{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 02. 의류 아이템 인식하기"
      ],
      "metadata": {
        "id": "aZxLpE-GMFG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 코드부터 작성하기\n",
        "\n",
        "구현 모델: 28x28 흑백 이미지를 입력받아 셔츠, 바지, 드레스 등의 종류 구별"
      ],
      "metadata": {
        "id": "x56StG8JMiUM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOhmVp9cIHt3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()"
      ],
      "metadata": {
        "id": "ga-_rEuEIpHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ff3f7c-d305-403a-a757-fb35e058b2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_images = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "3RYXKMcoI-17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "metadata": {
        "id": "ayVIhzZUI5T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "hj6qmeIkJmLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(training_images, training_labels, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cdi2zr_JvrP",
        "outputId": "bba18194-5830-41ee-b250-3baf78d8ca55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5039 - accuracy: 0.8228\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3801 - accuracy: 0.8621\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3376 - accuracy: 0.8762\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3138 - accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2967 - accuracy: 0.8911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 코드 분석하기"
      ],
      "metadata": {
        "id": "zNz9EVRQNMq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모듈 불러오기"
      ],
      "metadata": {
        "id": "GBlom7A3OTCg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COHKmxjJOMBI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 준비하기\n",
        "\n",
        "\n",
        "*   입력 데이터 = images\n",
        "*   출력 데이터 = labels"
      ],
      "metadata": {
        "id": "lI2zwVYkRNAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SAT55rvv11Y",
        "outputId": "899fab9e-2785-499b-8957-5c2042d424f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_images[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "5SJjtM3Ov5Ti",
        "outputId": "c37d1f00-8b6c-47a6-822f-1fc908053c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-777f8b51-bd72-40ff-969a-47cac6d77c9d\" class=\"ndarray_repr\"><pre>ndarray (28, 28) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACBUlEQVR4nLXSz0tUURQH8O+597373rx545g6NmNJY6TlJgozEsKFZGCrkDBoVdGmdf9BiwJ3bVr1PwhBUbQfJQcKjWZRTsjo0KDpDPV+eN+997WK0Z2bvtsPfM+Bc4D/HwJycwBZAEAE6zAyfe5RFMQfFYiRAj+CXM/c2HK82VetVMM34RGUmCxz9v7yYnW9dnWyskyHR6azi72Jwep3ScVk9c7LLhKQrpRBSiI2n76puZFT3doUwH4pcmw/zpjrU2zw3dFt4XEWdvbKhpinzTBYt5bDH4qlLYO8cbKBWHOudDHluFtiLrLD0kmM2//6q9VFS+JLLDxv9GzMPV9v3XuzYgHEGSUGCngbRCLd4W6CxPCLHTDwVMkDA0y/qEa/lFFBoF2EEME8CAD6hsZK8+djlmSatuiXXsWfNh27NU6Yelro1bytPElRbaGaO1FGPfc7zPg9HvHlIaVD8AjID9y/+bgZ/6iP9ks7J/QZevh8w/cd2PlGs8CKt92R7MQEk0yA7GtWq9ETN3zRs7fpR7FaWi/3yXaipW1IjFnbaSM70N7dsRzbzbHd8aCx7+wmKskUO5esz0sPmvXYF67gBzoNfxptxb5stxM10iLg1pOTO23NhcUptW1hE6gFYYprC8QMMPNsMM+4pamVbps/HGkSsg+1Cv4d+0Jh//Sm3DjGix4rfwFoJNh2/0cDFgAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-777f8b51-bd72-40ff-969a-47cac6d77c9d button').onclick = (e) => {\n",
              "        document.querySelector('#id-777f8b51-bd72-40ff-969a-47cac6d77c9d').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-777f8b51-bd72-40ff-969a-47cac6d77c9d button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spMZja5vwAoC",
        "outputId": "d937d0ee-cfc0-44f8-a835-6a44c59b3565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, 3, 0], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 정규화하기\n",
        "\n",
        "0-255 값의 데이터를 0-1의 범위로 정규화"
      ],
      "metadata": {
        "id": "3WrV-ii9wb5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_images = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "RuRBCXVjwnVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_images[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQa7qTDcwqIV",
        "outputId": "c8fbf51b-0e27-4990-8030-465a9c06c80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.00392157, 0.        , 0.        ,\n",
              "        0.05098039, 0.28627451, 0.        , 0.        , 0.00392157,\n",
              "        0.01568627, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.00392157, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.01176471, 0.        , 0.14117647,\n",
              "        0.53333333, 0.49803922, 0.24313725, 0.21176471, 0.        ,\n",
              "        0.        , 0.        , 0.00392157, 0.01176471, 0.01568627,\n",
              "        0.        , 0.        , 0.01176471],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.02352941, 0.        , 0.4       ,\n",
              "        0.8       , 0.69019608, 0.5254902 , 0.56470588, 0.48235294,\n",
              "        0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.04705882, 0.03921569, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.60784314,\n",
              "        0.9254902 , 0.81176471, 0.69803922, 0.41960784, 0.61176471,\n",
              "        0.63137255, 0.42745098, 0.25098039, 0.09019608, 0.30196078,\n",
              "        0.50980392, 0.28235294, 0.05882353],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.00392157, 0.        , 0.27058824, 0.81176471,\n",
              "        0.8745098 , 0.85490196, 0.84705882, 0.84705882, 0.63921569,\n",
              "        0.49803922, 0.4745098 , 0.47843137, 0.57254902, 0.55294118,\n",
              "        0.34509804, 0.6745098 , 0.25882353],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.00392157, 0.00392157, 0.        , 0.78431373, 0.90980392,\n",
              "        0.90980392, 0.91372549, 0.89803922, 0.8745098 , 0.8745098 ,\n",
              "        0.84313725, 0.83529412, 0.64313725, 0.49803922, 0.48235294,\n",
              "        0.76862745, 0.89803922, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.71764706, 0.88235294,\n",
              "        0.84705882, 0.8745098 , 0.89411765, 0.92156863, 0.89019608,\n",
              "        0.87843137, 0.87058824, 0.87843137, 0.86666667, 0.8745098 ,\n",
              "        0.96078431, 0.67843137, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.75686275, 0.89411765,\n",
              "        0.85490196, 0.83529412, 0.77647059, 0.70588235, 0.83137255,\n",
              "        0.82352941, 0.82745098, 0.83529412, 0.8745098 , 0.8627451 ,\n",
              "        0.95294118, 0.79215686, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.01176471, 0.        , 0.04705882, 0.85882353, 0.8627451 ,\n",
              "        0.83137255, 0.85490196, 0.75294118, 0.6627451 , 0.89019608,\n",
              "        0.81568627, 0.85490196, 0.87843137, 0.83137255, 0.88627451,\n",
              "        0.77254902, 0.81960784, 0.20392157],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.02352941, 0.        , 0.38823529, 0.95686275, 0.87058824,\n",
              "        0.8627451 , 0.85490196, 0.79607843, 0.77647059, 0.86666667,\n",
              "        0.84313725, 0.83529412, 0.87058824, 0.8627451 , 0.96078431,\n",
              "        0.46666667, 0.65490196, 0.21960784],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.01568627,\n",
              "        0.        , 0.        , 0.21568627, 0.9254902 , 0.89411765,\n",
              "        0.90196078, 0.89411765, 0.94117647, 0.90980392, 0.83529412,\n",
              "        0.85490196, 0.8745098 , 0.91764706, 0.85098039, 0.85098039,\n",
              "        0.81960784, 0.36078431, 0.        ],\n",
              "       [0.        , 0.        , 0.00392157, 0.01568627, 0.02352941,\n",
              "        0.02745098, 0.00784314, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.92941176, 0.88627451, 0.85098039,\n",
              "        0.8745098 , 0.87058824, 0.85882353, 0.87058824, 0.86666667,\n",
              "        0.84705882, 0.8745098 , 0.89803922, 0.84313725, 0.85490196,\n",
              "        1.        , 0.30196078, 0.        ],\n",
              "       [0.        , 0.01176471, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.24313725,\n",
              "        0.56862745, 0.8       , 0.89411765, 0.81176471, 0.83529412,\n",
              "        0.86666667, 0.85490196, 0.81568627, 0.82745098, 0.85490196,\n",
              "        0.87843137, 0.8745098 , 0.85882353, 0.84313725, 0.87843137,\n",
              "        0.95686275, 0.62352941, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.07058824,\n",
              "        0.17254902, 0.32156863, 0.41960784, 0.74117647, 0.89411765,\n",
              "        0.8627451 , 0.87058824, 0.85098039, 0.88627451, 0.78431373,\n",
              "        0.80392157, 0.82745098, 0.90196078, 0.87843137, 0.91764706,\n",
              "        0.69019608, 0.7372549 , 0.98039216, 0.97254902, 0.91372549,\n",
              "        0.93333333, 0.84313725, 0.        ],\n",
              "       [0.        , 0.22352941, 0.73333333, 0.81568627, 0.87843137,\n",
              "        0.86666667, 0.87843137, 0.81568627, 0.8       , 0.83921569,\n",
              "        0.81568627, 0.81960784, 0.78431373, 0.62352941, 0.96078431,\n",
              "        0.75686275, 0.80784314, 0.8745098 , 1.        , 1.        ,\n",
              "        0.86666667, 0.91764706, 0.86666667, 0.82745098, 0.8627451 ,\n",
              "        0.90980392, 0.96470588, 0.        ],\n",
              "       [0.01176471, 0.79215686, 0.89411765, 0.87843137, 0.86666667,\n",
              "        0.82745098, 0.82745098, 0.83921569, 0.80392157, 0.80392157,\n",
              "        0.80392157, 0.8627451 , 0.94117647, 0.31372549, 0.58823529,\n",
              "        1.        , 0.89803922, 0.86666667, 0.7372549 , 0.60392157,\n",
              "        0.74901961, 0.82352941, 0.8       , 0.81960784, 0.87058824,\n",
              "        0.89411765, 0.88235294, 0.        ],\n",
              "       [0.38431373, 0.91372549, 0.77647059, 0.82352941, 0.87058824,\n",
              "        0.89803922, 0.89803922, 0.91764706, 0.97647059, 0.8627451 ,\n",
              "        0.76078431, 0.84313725, 0.85098039, 0.94509804, 0.25490196,\n",
              "        0.28627451, 0.41568627, 0.45882353, 0.65882353, 0.85882353,\n",
              "        0.86666667, 0.84313725, 0.85098039, 0.8745098 , 0.8745098 ,\n",
              "        0.87843137, 0.89803922, 0.11372549],\n",
              "       [0.29411765, 0.8       , 0.83137255, 0.8       , 0.75686275,\n",
              "        0.80392157, 0.82745098, 0.88235294, 0.84705882, 0.7254902 ,\n",
              "        0.77254902, 0.80784314, 0.77647059, 0.83529412, 0.94117647,\n",
              "        0.76470588, 0.89019608, 0.96078431, 0.9372549 , 0.8745098 ,\n",
              "        0.85490196, 0.83137255, 0.81960784, 0.87058824, 0.8627451 ,\n",
              "        0.86666667, 0.90196078, 0.2627451 ],\n",
              "       [0.18823529, 0.79607843, 0.71764706, 0.76078431, 0.83529412,\n",
              "        0.77254902, 0.7254902 , 0.74509804, 0.76078431, 0.75294118,\n",
              "        0.79215686, 0.83921569, 0.85882353, 0.86666667, 0.8627451 ,\n",
              "        0.9254902 , 0.88235294, 0.84705882, 0.78039216, 0.80784314,\n",
              "        0.72941176, 0.70980392, 0.69411765, 0.6745098 , 0.70980392,\n",
              "        0.80392157, 0.80784314, 0.45098039],\n",
              "       [0.        , 0.47843137, 0.85882353, 0.75686275, 0.70196078,\n",
              "        0.67058824, 0.71764706, 0.76862745, 0.8       , 0.82352941,\n",
              "        0.83529412, 0.81176471, 0.82745098, 0.82352941, 0.78431373,\n",
              "        0.76862745, 0.76078431, 0.74901961, 0.76470588, 0.74901961,\n",
              "        0.77647059, 0.75294118, 0.69019608, 0.61176471, 0.65490196,\n",
              "        0.69411765, 0.82352941, 0.36078431],\n",
              "       [0.        , 0.        , 0.29019608, 0.74117647, 0.83137255,\n",
              "        0.74901961, 0.68627451, 0.6745098 , 0.68627451, 0.70980392,\n",
              "        0.7254902 , 0.7372549 , 0.74117647, 0.7372549 , 0.75686275,\n",
              "        0.77647059, 0.8       , 0.81960784, 0.82352941, 0.82352941,\n",
              "        0.82745098, 0.7372549 , 0.7372549 , 0.76078431, 0.75294118,\n",
              "        0.84705882, 0.66666667, 0.        ],\n",
              "       [0.00784314, 0.        , 0.        , 0.        , 0.25882353,\n",
              "        0.78431373, 0.87058824, 0.92941176, 0.9372549 , 0.94901961,\n",
              "        0.96470588, 0.95294118, 0.95686275, 0.86666667, 0.8627451 ,\n",
              "        0.75686275, 0.74901961, 0.70196078, 0.71372549, 0.71372549,\n",
              "        0.70980392, 0.69019608, 0.65098039, 0.65882353, 0.38823529,\n",
              "        0.22745098, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,\n",
              "        0.28235294, 0.16078431, 0.1372549 , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 구성하기\n",
        "\n",
        "입력(28x28) </br>\n",
        "-> Flatten(입력된 2차원 데이터를 1차원 배열로 재구성), 784 </br>\n",
        "-> Dense(입력된 784 길이의 배열을 연산 후 128 길이의 배열로 출력) + Relu, 128 </br>\n",
        "-> Dense(입력된 128길이의 배열을 연산 후 softmax를 통해 10개의 확률로 출력) + SoftMax, 10 </br>\n",
        "-> 출력(10)\n",
        "\n",
        "\n",
        "*   **옵티마이저(optimizer)**: Adam(Adaptive Moment Estimation)\n",
        "*   **손실함수(loss, Loss Function)**: sparse_categorical_crossentropy(0,1,2,3 등의 정수 값의 라벨에 대한 다중 분류 손실함수)"
      ],
      "metadata": {
        "id": "hUWhGt9IPE_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "metadata": {
        "id": "0wQ1fw4hOMBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jrA_6bBQOMBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습하기\n",
        "tf.keras.Model.fit(x, y)\n",
        "\n",
        "\n",
        "*   Input = training_images\n",
        "*   ouput = training_labels\n",
        "*   epochs = 학습의 반복 수\n",
        "\n"
      ],
      "metadata": {
        "id": "sB_gTDLbmRyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(training_images, training_labels, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f7579d-148f-4a42-d91f-547de5c9980d",
        "id": "JzuDXG2TOMBL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4970 - accuracy: 0.8252\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3776 - accuracy: 0.8646\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3377 - accuracy: 0.8773\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3127 - accuracy: 0.8858\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.2971 - accuracy: 0.8894\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b949e9421d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예측하기"
      ],
      "metadata": {
        "id": "D0GUYHjkmyXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ce08ad-60a6-4769-ff68-d225a3ab4a95",
        "id": "WfnfwBanOMBM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3715 - accuracy: 0.8633\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37151914834976196, 0.8633000254631042]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* loss(error, 손실) = 0.37\n",
        "* accuracy(정확도) = 0.86"
      ],
      "metadata": {
        "id": "wOJQG4xgzdY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 출력 살펴보기"
      ],
      "metadata": {
        "id": "DcyW0kdTze9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piaCkpbrznZn",
        "outputId": "ee1b4e19-8423-458f-c9bf-e62ee44801bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n",
            "[5.9600575e-06 1.0351787e-07 2.6862290e-06 3.9447536e-07 2.4955700e-05\n",
            " 6.7921299e-03 1.3235795e-05 4.8533618e-01 1.6399517e-03 5.0618428e-01]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델은 라벨 0-9 에대한 확률을 출력한다. 라벨 9의 확률이 0.506으로 가장 높아 실제 정답인 9와 일치한다."
      ],
      "metadata": {
        "id": "x0EYzh4ezxiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfitting (과대적합)\n",
        "\n",
        "epoch를 50번으로 늘려 과대적합 현상을 알아보자."
      ],
      "metadata": {
        "id": "o1ptoz6g0OYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(training_images, training_labels, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncvZPfQr0moT",
        "outputId": "6f8c49d8-7319-4fe2-eeab-1e2ada8ea7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2807 - accuracy: 0.8970\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2684 - accuracy: 0.9014\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2603 - accuracy: 0.9030\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2488 - accuracy: 0.9071\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2409 - accuracy: 0.9113\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2316 - accuracy: 0.9130\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2252 - accuracy: 0.9158\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2182 - accuracy: 0.9188\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2135 - accuracy: 0.9202\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.2053 - accuracy: 0.9237\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2002 - accuracy: 0.9249\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1954 - accuracy: 0.9260\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1891 - accuracy: 0.9282\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1848 - accuracy: 0.9311\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1793 - accuracy: 0.9334\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1760 - accuracy: 0.9355\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1709 - accuracy: 0.9349\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1664 - accuracy: 0.9380\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1615 - accuracy: 0.9395\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1580 - accuracy: 0.9406\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1549 - accuracy: 0.9422\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1530 - accuracy: 0.9428\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1474 - accuracy: 0.9446\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1452 - accuracy: 0.9454\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1424 - accuracy: 0.9464\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1391 - accuracy: 0.9474\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1353 - accuracy: 0.9497\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1341 - accuracy: 0.9492\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1320 - accuracy: 0.9509\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1265 - accuracy: 0.9520\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1273 - accuracy: 0.9524\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1216 - accuracy: 0.9541\n",
            "Epoch 33/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1223 - accuracy: 0.9539\n",
            "Epoch 34/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1195 - accuracy: 0.9547\n",
            "Epoch 35/50\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.1180 - accuracy: 0.9559\n",
            "Epoch 36/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1138 - accuracy: 0.9575\n",
            "Epoch 37/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1137 - accuracy: 0.9570\n",
            "Epoch 38/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1107 - accuracy: 0.9588\n",
            "Epoch 39/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1061 - accuracy: 0.9599\n",
            "Epoch 40/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1077 - accuracy: 0.9588\n",
            "Epoch 41/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1053 - accuracy: 0.9606\n",
            "Epoch 42/50\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.1023 - accuracy: 0.9618\n",
            "Epoch 43/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1009 - accuracy: 0.9620\n",
            "Epoch 44/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1005 - accuracy: 0.9617\n",
            "Epoch 45/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0986 - accuracy: 0.9631\n",
            "Epoch 46/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0960 - accuracy: 0.9642\n",
            "Epoch 47/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0963 - accuracy: 0.9633\n",
            "Epoch 48/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0932 - accuracy: 0.9653\n",
            "Epoch 49/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0936 - accuracy: 0.9656\n",
            "Epoch 50/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0901 - accuracy: 0.9664\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b949e7dd1b0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjOd1LPI4l_e",
        "outputId": "526b5a43-d933-48f9-db3d-b058593352c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.8902\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5398319363594055, 0.8902000188827515]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정확도가 0.89였던 훈련데이터에 대해서는 0.96으로 크게 올랐지만 테스트 데이터의 정확도는 0.86에서 0.89로 소폭 성장하였다. 따라서 학습 epoch에 비해 낮은 성능을 보이고 있다."
      ],
      "metadata": {
        "id": "oFGTdPfI4wra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callback으로 과학습 방지하기\n",
        "\n",
        "텐서플로우의 Callback 함수를 이용하여 학습 도중 95% 이상의 정확도를 보이면 학습을 중지하도록 설계한다."
      ],
      "metadata": {
        "id": "NsKKkHHd-Ezf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.95):\n",
        "      print(\"\\n 정확도 95%에 도달하여 훈련을 중지합니다.\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "model2 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks = myCallback()\n",
        "model2.fit(training_images, training_labels, epochs=50, callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrUwivq-5Pmf",
        "outputId": "3c7d1fc5-7ed0-4ac8-a35f-77de393c0b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.4991 - accuracy: 0.8238\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3759 - accuracy: 0.8633\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3390 - accuracy: 0.8764\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3116 - accuracy: 0.8854\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2944 - accuracy: 0.8916\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2787 - accuracy: 0.8966\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2673 - accuracy: 0.9002\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2549 - accuracy: 0.9054\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.2472 - accuracy: 0.9087\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.2360 - accuracy: 0.9116\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2301 - accuracy: 0.9135\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2218 - accuracy: 0.9174\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2145 - accuracy: 0.9196\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2092 - accuracy: 0.9227\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2032 - accuracy: 0.9242\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1974 - accuracy: 0.9251\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1929 - accuracy: 0.9280\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1868 - accuracy: 0.9303\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1823 - accuracy: 0.9308\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.1779 - accuracy: 0.9330\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1721 - accuracy: 0.9349\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1695 - accuracy: 0.9369\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.1644 - accuracy: 0.9382\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1610 - accuracy: 0.9396\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1601 - accuracy: 0.9394\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1548 - accuracy: 0.9415\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1505 - accuracy: 0.9434\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1486 - accuracy: 0.9451\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1445 - accuracy: 0.9448\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1430 - accuracy: 0.9460\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1407 - accuracy: 0.9466\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1373 - accuracy: 0.9481\n",
            "Epoch 33/50\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.1320 - accuracy: 0.9506\n",
            " 정확도 95%에 도달하여 훈련을 중지합니다.\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1320 - accuracy: 0.9506\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b947c12d450>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}